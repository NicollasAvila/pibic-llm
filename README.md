
````markdown
# Benchmarking de SLMs para Resposta a Incidentes (PIBIC)

Este reposit√≥rio cont√©m os scripts e dados utilizados na pesquisa de Inicia√ß√£o Cient√≠fica (UEPA) sobre o uso de **Small Language Models (SLMs)** locais para a gera√ß√£o autom√°tica de Playbooks de Resposta a Incidentes de Ciberseguran√ßa.

O objetivo √© validar a capacidade de modelos leves (rodando em CPU/Notebooks) de interpretar logs de seguran√ßa (JSON) e gerar planos de a√ß√£o t√©cnicos.

## üìã Pr√©-requisitos

Para rodar este projeto, voc√™ precisar√° de:

1.  **Python 3.10+** instalado.
2.  **[Ollama](https://ollama.com/)** instalado e rodando em segundo plano (essencial para gerenciar os modelos).
3.  **Git** para clonar o reposit√≥rio.

## üöÄ Instala√ß√£o e Configura√ß√£o

Siga os passos abaixo para preparar o ambiente de desenvolvimento.

### 1. Clonar o Reposit√≥rio

```bash
git clone [URL_DO_SEU_REPOSITORIO]
cd [NOME_DA_PASTA]
````

### 2\. Criar e Ativar o Ambiente Virtual

Isolamos as depend√™ncias do projeto para evitar conflitos.

**No Windows (PowerShell):**

```bash
python -m venv .venv
.\.venv\Scripts\activate
```

**No Linux/Mac:**

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3\. Instalar Depend√™ncias

Instale as bibliotecas Python necess√°rias (`ollama`, `litellm`, etc.):

```bash
pip install -r requirements.txt
```

### 4\. Baixar os Modelos de IA (Ollama)

Este projeto compara diferentes modelos. Execute os comandos abaixo no terminal para baixar os "c√©rebros" das IAs para sua m√°quina:

```bash
# Modelo leve (3B) - Para testes r√°pidos
ollama pull llama3.2

# Modelos robustos (7B/8B) - Para o benchmark comparativo
ollama pull llama3.1
ollama pull mistral
ollama pull qwen2.5
```

-----

## üìÇ Estrutura do Projeto

  * **`dados/`**: Cont√©m os arquivos de log brutos (`log1.json`, `log2.json`) simulando eventos de seguran√ßa (ex: detec√ß√£o de PowerShell malicioso).
  * **`gerar_playbook.py`**: Script para teste r√°pido. Gera um √∫nico playbook no terminal usando o modelo mais leve (`llama3.2`).
  * **`comparar_modelos.py`**: Script de pesquisa. Executa uma bateria de testes com 3 modelos diferentes (`llama3.1`, `mistral`, `qwen2.5`), cronometra o tempo e salva os resultados em arquivos de texto.

-----

## üß™ Como Rodar os Testes

Certifique-se de que o aplicativo **Ollama** est√° aberto e rodando perto do rel√≥gio do sistema.

### Teste 1: Valida√ß√£o R√°pida (Terminal)

Para ver se o sistema est√° funcionando e gerar um playbook instant√¢neo na tela:

```bash
python gerar_playbook.py
```

*Modelo usado:* Llama 3.2 (3B)

### Teste 2: Benchmark Comparativo (Pesquisa)

Para rodar a compara√ß√£o entre Llama 3.1, Mistral e Qwen. Este processo pode levar alguns minutos dependendo do hardware.

```bash
python comparar_modelos.py
```

**Sa√≠da esperada:**
O script criar√° arquivos `.txt` na pasta raiz com o nome de cada modelo (ex: `resultado_mistral.txt`), contendo:

  * O tempo total de execu√ß√£o.
  * O Playbook gerado pelo modelo.

-----

## üìä Resultados Preliminares (Notebook)

Testes realizados em ambiente de Notebook (CPU):

| Modelo | Par√¢metros | Tempo M√©dio | Observa√ß√£o |
| :--- | :--- | :--- | :--- |
| **Llama 3.2** | 3B | \~2.5 min | R√°pido, ideal para dev. |
| **Qwen 2.5** | 7B | \~6.1 min | Melhor performance entre os 7B. |
| **Mistral** | 7B | \~6.2 min | Respostas consistentes. |
| **Llama 3.1** | 8B | \~6.2 min | Padr√£o de mercado. |

-----



<!-- end list -->

```
```